[67.6, 69.2, 71.2, 74.0, 76.0]


{'loss': 0.29379999999999995, 'bias': 0.292, 'var': 0.0017999999999999683, 'varp': 0.03700000000000001, 'varn': 0.035200000000000016, 'varc': 0.03520000000000001}
{'loss': 0.30940000000000006, 'bias': 0.28, 'var': 0.029400000000000037, 'varp': 0.09159999999999999, 'varn': 0.062199999999999984, 'varc': 0.06219999999999997}
{'loss': 0.30932000000000004, 'bias': 0.288, 'var': 0.02132000000000006, 'varp': 0.09347999999999997, 'varn': 0.07215999999999997, 'varc': 0.07215999999999997}
{'loss': 0.3205999999999998, 'bias': 0.284, 'var': 0.0365999999999998, 'varp': 0.11532000000000008, 'varn': 0.07872, 'varc': 0.07872000000000001}
{'loss': 0.32835999999999993, 'bias': 0.28, 'var': 0.0483599999999999, 'varp': 0.12696000000000002, 'varn': 0.07859999999999998, 'varc': 0.07859999999999998}




[0.332, 0.584, 0.332]
[0.0, -0.06786666666666663, 0.0]
Total execution time: 617.59 secs
~/Workspace/PMPMS/ML/HW4 $ python3 main.py data_hw4_diabetes/diabetes_train.txt data_hw4_diabetes/diabetes_test.txt
Loading Train File: data_hw4_diabetes/diabetes_train.txt
Training Loaded: 0.002 secs!

Loading Test File: data_hw4_diabetes/diabetes_test.txt
Test Loaded: 0.001secs!

/usr/local/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)
[0.268]
[-0.0022666666666666946]







 $ python3 main.py data_hw4_diabetes/diabetes_train.txt data_hw4_diabetes/diabetes_test.txt
Loading Train File: data_hw4_diabetes/diabetes_train.txt
Training Loaded: 0.002 secs!

Loading Test File: data_hw4_diabetes/diabetes_test.txt
Test Loaded: 0.001secs!

Samplings =  1
[0.28, 0.272, 0.272, 0.26, 0.26, 0.28, 0.284, 0.276, 0.28, 0.272, 0.268, 0.28, 0.296, 0.288, 0.272, 0.276, 0.284, 0.272, 0.268, 0.268]
[0.030520000000000103, 0.029120000000000257, 0.04187999999999992, 0.04619999999999991, 0.05316000000000032, 0.04620000000000013, 0.043399999999999994, 0.05228000000000005, 0.052479999999999916, 0.05956000000000017, 0.0613200000000001, 0.04811999999999994, 0.04116000000000003, 0.048280000000000045, 0.05923999999999996, 0.053720000000000157, 0.04672000000000015, 0.05823999999999996, 0.06119999999999998, 0.06095999999999979]

Samplings =  3
[0.28, 0.28, 0.284, 0.272, 0.284, 0.272, 0.268, 0.272, 0.304, 0.284, 0.284, 0.272, 0.288, 0.284, 0.276, 0.28, 0.268, 0.284, 0.276, 0.28]
[0.015720000000000012, 0.00768000000000002, 0.018800000000000094, 0.02260000000000001, 0.012640000000000096, 0.031000000000000083, 0.03859999999999986, 0.037840000000000096, 0.009200000000000208, 0.02692, 0.031160000000000132, 0.038839999999999986, 0.02303999999999995, 0.028079999999999994, 0.03963999999999984, 0.036200000000000176, 0.04555999999999999, 0.02951999999999988, 0.037200000000000066, 0.03148000000000012]

Samplings =  5
[0.288, 0.288, 0.276, 0.28, 0.272, 0.26, 0.284, 0.272, 0.268, 0.272, 0.268, 0.288, 0.284, 0.284, 0.296, 0.28, 0.28, 0.272, 0.272, 0.276]
[-0.00032000000000004247, -0.0037600000000000966, 0.020160000000000122, 0.010880000000000334, 0.01980000000000004, 0.04023999999999994, 0.01796000000000031, 0.027840000000000253, 0.03408000000000022, 0.032879999999999854, 0.03647999999999996, 0.016519999999999868, 0.017800000000000038, 0.020880000000000065, 0.010440000000000338, 0.025080000000000158, 0.028679999999999928, 0.03631999999999985, 0.03416000000000008, 0.028239999999999932]

Samplings =  10
[0.28, 0.276, 0.284, 0.28, 0.288, 0.276, 0.28, 0.284, 0.256, 0.284, 0.272, 0.28, 0.28, 0.284, 0.272, 0.272, 0.276, 0.284, 0.296, 0.288]
[0.015199999999999936, 0.005919999999999981, 0.011560000000000126, 0.007279999999999676, 0.0034000000000000696, 0.014400000000000246, 0.012600000000000056, 0.015120000000000133, 0.041400000000000214, 0.014479999999999993, 0.02924000000000021, 0.019240000000000146, 0.01788000000000023, 0.016840000000000244, 0.02540000000000009, 0.023120000000000085, 0.02051999999999987, 0.015040000000000275, 0.0044000000000001815, 0.01060000000000022]

Samplings =  20
[0.288, 0.272, 0.276, 0.268, 0.276, 0.272, 0.292, 0.276, 0.284, 0.284, 0.276, 0.268, 0.28, 0.276, 0.272, 0.28, 0.288, 0.276, 0.28, 0.284]
[0.0015600000000001168, 0.010680000000000023, 0.014119999999999966, 0.015240000000000087, 0.010079999999999922, 0.015880000000000227, -0.00047999999999992493, 0.015239999999999976, 0.008759999999999879, 0.013040000000000052, 0.019480000000000053, 0.02520000000000011, 0.011040000000000105, 0.01968000000000003, 0.021959999999999924, 0.013039999999999996, 0.0077600000000001, 0.018399999999999916, 0.01428000000000007, 0.010920000000000096]